{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951b03f9",
   "metadata": {},
   "source": [
    "In the previous ``preprocessing`` notebook, we performed several important steps:\n",
    "-  we created **separate** tables for each currency, ensuring that the data is organized in a structured manner;\n",
    "- additionally, we **quantized** the time series data by applying aggregations over a ``300ms`` time window, this **reduced** the data size, **unified** the data format across all currencies, and simplified the problem to predicting the average price within the next ``300ms`` window.\n",
    "\n",
    "Now, in the ``features_creation`` notebook, our focus will be on constructing **new and valuable** features. We will explore popular approaches to feature creation and leverage the insights gained from the target time series. This will allow us to identify which features are most likely to be useful for our task. In the subsequent ``features_selection`` notebook, we will incorporate information from **other currencies** and perform a feature selection process to choose the most impactful features.\n",
    "\n",
    "By following this approach, we aim to build a robust set of features that will contribute to the accuracy and effectiveness of our predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9887a14",
   "metadata": {},
   "source": [
    "# Solution description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e9185",
   "metadata": {},
   "source": [
    "**Target:** predict the price of the target currency for the next `300ms`\n",
    "\n",
    "**Input:** transaction history for a set of currencies (including the target one)\n",
    "\n",
    "**Limitations:** fast inference and model learning; small number of features\n",
    "\n",
    "First, we will generate a huge number of statistical features (based on the target table). Then we will use statistical hypotheses to test whether they are important in the forecasting. We discard all unimportant features, and thus obtain `stats_selected_features`. This part will be solved using methods from `tsfresh` library and ``block cross validation`` of time series. I use block cross-validation to test the importance of features in different time conditions within the same day. You can read more about what block cross validation is in the **DOCS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261c3954",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import extraction_utils\n",
    "import preprocessing_utils\n",
    "from tsfresh.feature_extraction import EfficientFCParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7872b84",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "names = np.array([\n",
    "    '1000LUNC_USDT_PERP', '1000SHIB_USDT_PERP', '1000XEC_USDT_PERP',\n",
    "    '1INCH_USDT_PERP', 'AAVE_USDT_PERP', 'ADA_BUSD_PERP', 'ADA_USDT_PERP',\n",
    "    'ALGO_USDT_PERP', 'ALICE_USDT_PERP', 'ALPHA_USDT_PERP', 'ANC_BUSD_PERP',\n",
    "    'ANKR_USDT_PERP', 'ANT_USDT_PERP', 'APE_BUSD_PERP', 'APE_USDT_PERP',\n",
    "    'API3_USDT_PERP', 'APT_USDT_PERP', 'ARPA_USDT_PERP', 'AR_USDT_PERP',\n",
    "    'ATA_USDT_PERP', 'ATOM_USDT_PERP', 'AUDIO_USDT_PERP', 'AVAX_BUSD_PERP',\n",
    "    'AVAX_USDT_PERP', 'AXS_USDT_PERP', 'BAKE_USDT_PERP', 'BAL_USDT_PERP',\n",
    "    'BAND_USDT_PERP', 'BAT_USDT_PERP', 'BCH_USDT_PERP', 'BEL_USDT_PERP',\n",
    "    'BLZ_USDT_PERP', 'BNB_BUSD_PERP', 'BNB_USDT_PERP', 'BNX_USDT_PERP',\n",
    "    'BTCDOM_USDT_PERP', 'BTC_BUSD_PERP', 'BTC_USDT_CQ', 'BTC_USDT_PERP',\n",
    "    'C98_USDT_PERP', 'CELO_USDT_PERP', 'CELR_USDT_PERP', 'CHR_USDT_PERP',\n",
    "    'CHZ_USDT_PERP', 'COMP_USDT_PERP', 'COTI_USDT_PERP', 'CRV_USDT_PERP',\n",
    "    'CTK_USDT_PERP', 'CTSI_USDT_PERP', 'CVC_USDT_PERP', 'CVX_USDT_PERP',\n",
    "    'DAR_USDT_PERP', 'DASH_USDT_PERP', 'DEFI_USDT_PERP', 'DENT_USDT_PERP',\n",
    "    'DGB_USDT_PERP', 'DODO_BUSD_PERP', 'DOGE_BUSD_PERP', 'DOGE_USDT_PERP',\n",
    "    'DOT_BUSD_PERP', 'DOT_USDT_PERP', 'DUSK_USDT_PERP', 'DYDX_USDT_PERP',\n",
    "    'EGLD_USDT_PERP', 'ENJ_USDT_PERP', 'ENS_USDT_PERP', 'EOS_USDT_PERP',\n",
    "    'ETC_BUSD_PERP', 'ETC_USDT_PERP', 'ETH_BUSD_PERP', 'ETH_USDT_CQ',\n",
    "    'ETH_USDT_PERP', 'FIL_BUSD_PERP', 'FIL_USDT_PERP', 'FLM_USDT_PERP',\n",
    "    'FLOW_USDT_PERP', 'FOOTBALL_USDT_PERP', 'FTM_BUSD_PERP', 'FTM_USDT_PERP',\n",
    "    'GALA_BUSD_PERP', 'GALA_USDT_PERP', 'GAL_BUSD_PERP', 'GAL_USDT_PERP',\n",
    "    'GMT_BUSD_PERP', 'GMT_USDT_PERP', 'GRT_USDT_PERP', 'GTC_USDT_PERP',\n",
    "    'HBAR_USDT_PERP', 'HNT_USDT_PERP', 'HOT_USDT_PERP', 'ICP_BUSD_PERP',\n",
    "    'ICP_USDT_PERP', 'ICX_USDT_PERP', 'IMX_USDT_PERP', 'INJ_USDT_PERP',\n",
    "    'IOST_USDT_PERP', 'IOTA_USDT_PERP', 'IOTX_USDT_PERP', 'JASMY_USDT_PERP',\n",
    "    'KAVA_USDT_PERP', 'KLAY_USDT_PERP', 'KNC_USDT_PERP', 'KSM_USDT_PERP',\n",
    "    'LDO_USDT_PERP', 'LINA_USDT_PERP', 'LINK_BUSD_PERP', 'LINK_USDT_PERP',\n",
    "    'LIT_USDT_PERP', 'LPT_USDT_PERP', 'LRC_USDT_PERP', 'LTC_USDT_PERP',\n",
    "    'LUNA2_USDT_PERP', 'MANA_USDT_PERP', 'MASK_USDT_PERP', 'MATIC_BUSD_PERP',\n",
    "    'MATIC_USDT_PERP', 'MKR_USDT_PERP', 'MTL_USDT_PERP', 'NEAR_BUSD_PERP',\n",
    "    'NEAR_USDT_PERP', 'NEO_USDT_PERP', 'NKN_USDT_PERP', 'OCEAN_USDT_PERP',\n",
    "    'OGN_USDT_PERP', 'OMG_USDT_PERP', 'ONE_USDT_PERP', 'ONT_USDT_PERP',\n",
    "    'OP_USDT_PERP', 'PEOPLE_USDT_PERP', 'QNT_USDT_PERP', 'QTUM_USDT_PERP',\n",
    "    'RAY_USDT_PERP', 'REEF_USDT_PERP', 'REN_USDT_PERP', 'RLC_USDT_PERP',\n",
    "    'ROSE_USDT_PERP', 'RSR_USDT_PERP', 'RUNE_USDT_PERP', 'RVN_USDT_PERP',\n",
    "    'SAND_USDT_PERP', 'SFP_USDT_PERP', 'SKL_USDT_PERP', 'SNX_USDT_PERP',\n",
    "    'SOL_BUSD_PERP', 'SOL_USDT_PERP', 'SPELL_USDT_PERP', 'SRM_USDT_PERP',\n",
    "    'STG_USDT_PERP', 'STMX_USDT_PERP', 'STORJ_USDT_PERP', 'SUSHI_USDT_PERP',\n",
    "    'SXP_USDT_PERP', 'THETA_USDT_PERP', 'TOMO_USDT_PERP', 'TRB_USDT_PERP',\n",
    "    'TRX_BUSD_PERP', 'TRX_USDT_PERP', 'UNFI_USDT_PERP', 'UNI_BUSD_PERP',\n",
    "    'UNI_USDT_PERP', 'VET_USDT_PERP', 'WAVES_BUSD_PERP', 'WAVES_USDT_PERP',\n",
    "    'WOO_USDT_PERP', 'XEM_USDT_PERP', 'XLM_USDT_PERP', 'XMR_USDT_PERP',\n",
    "    'XRP_BUSD_PERP', 'XRP_USDT_PERP', 'XTZ_USDT_PERP', 'YFI_USDT_PERP',\n",
    "    'ZEC_USDT_PERP', 'ZEN_USDT_PERP', 'ZIL_USDT_PERP', 'ZRX_USDT_PERP'\n",
    "],\n",
    "                 dtype=object)\n",
    "TARGET_NAME = 'CHZ_USDT_PERP_MIDPRICE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35de4dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantized_df_dict = preprocessing_utils.load_tables(names=list(names)+[TARGET_NAME],\n",
    "                                              path_from='../data/quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "541cd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = quantized_df_dict[TARGET_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "084fda83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-15 00:00:00.000</th>\n",
       "      <td>0.198392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15 00:00:00.300</th>\n",
       "      <td>0.198341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15 00:00:00.600</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         price_mean\n",
       "event_time                         \n",
       "2022-11-15 00:00:00.000    0.198392\n",
       "2022-11-15 00:00:00.300    0.198341\n",
       "2022-11-15 00:00:00.600         NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fca4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adce32c",
   "metadata": {},
   "source": [
    "# Block cross validation and feature building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2ec6e",
   "metadata": {},
   "source": [
    "Let's do a block cross-validation. The number of blocks will be 24 (approximately one block for each hour). For each block we will take 1800 (this is approximately the last 9 minutes) windows with a width of a minute.\n",
    "\n",
    "Since we have not yet reduced the dimensionality of the features to be counted (`tsfresh` generates about 800 features), the next step takes quite a long time. \\\n",
    "*You can go drink ``./shop/coffee`` :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d321080",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_list = extraction_utils.bcv_extract_features(\n",
    "    df=target_table,\n",
    "    n_blocks=24, \n",
    "    target_col='price_mean',\n",
    "    n_jobs=8,\n",
    "    n_windows=1800,\n",
    "    window_size=200,\n",
    "    lags=list(range(1,11)),\n",
    "    mode='parallel',\n",
    "    fc_parameters=EfficientFCParameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c653bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 793)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45080aa7",
   "metadata": {},
   "source": [
    "# Size reduction\n",
    "\n",
    "### using some techniques, we will select the most important features among all those generated by ``tsfresh``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0184d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7af0bd",
   "metadata": {},
   "source": [
    "Note that the time interval of our predictions is `300ms`. At the same time we are not able to train our model on-line. This leads us to think that the model we built will be used in practice for **more time**. So we need to estimate the quality of its predictions not only for the next `300ms`, but for several steps ahead. \n",
    "\n",
    "For simplicity, we will take 10 intervals of `300ms`, that is, our model will predict the prices behavior for the next 3 seconds. During this time we are already able to train a new model on a part of the received data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899cdd75",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_rmse(\n",
    "        models: List[xgboost.sklearn.XGBRegressor],\n",
    "        test_list: List[pd.DataFrame]\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Predictions are made for each model in the `models` and the corresponding\n",
    "    training dataset. Then rmse are calculated for all lengths of the\n",
    "    prediction horizon from 1 to `len(test_list)`. That is, we look at the\n",
    "    rmse of the predictions for the next `300ms`, the next `600ms`, and so on.  \n",
    "    \n",
    "    :param models: list of trained XGBRegressors\n",
    "    :param test_list: test data list\n",
    "    :return: list containing the list with rmse for all prediction horizons\n",
    "    \"\"\"\n",
    "    assert len(models) == len(test_list), f'len(models)={len(models)}!={len(test_list)}=len(test_list)!'\n",
    "\n",
    "    length = test_list[0].shape[0]\n",
    "\n",
    "    losses = []\n",
    "    for i, model in enumerate(models):\n",
    "        losses.append([])\n",
    "        x, y = test_list[i].drop(['target'], axis=1), test_list[i].loc[:, 'target']\n",
    "        mse = .0\n",
    "        for l in range(length):\n",
    "            y_hat = model.predict(x.iloc[l:l + 1])\n",
    "            mse += (y_hat - y.iloc[l]) ** 2\n",
    "            losses[-1].append(np.sqrt(mse / (l + 1)))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6e70f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selection_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6839509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88db0a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# default features from tsfresh\n",
    "def_start_time = time.time()\n",
    "def_train = [df[:-test_size] for df in train_list]\n",
    "def_test = [df[-test_size:] for df in train_list]\n",
    "def_models = selection_utils.get_fitted_models(def_train, n_jobs)\n",
    "def_rmse = get_rmse(def_models, def_test)\n",
    "def_time = time.time() - def_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575f162",
   "metadata": {},
   "source": [
    "Now we will try to reduce the dimensionality of the feature space by calculating the statistical significance of each of them. After that we will select the uncorrelated features with the highest `p_value`. This will help us reduce the dimensionality of the space even more, and will also help us use the `feature_importance` technique in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49d90dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsfresh.feature_selection.relevance import calculate_relevance_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a16d8e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_stats in module selection_utils:\n",
      "\n",
      "get_stats(blocks, n_jobs=1)\n",
      "    Using statistical criteria, calculates the significance of the features\n",
      "    for each block in the list. Then the obtained ``p_value`` s are averaged.\n",
      "\n",
      "    :param blocks: List[pd.DataFrame]: list of datas with ``target`` column and the same scheme\n",
      "    :param n_jobs: int: the number of cores that can be used in the calculation of stat values\n",
      "    :return: pd.DataFrame: df with calculated ``p_value`` for each of the attributes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.get_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2883992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stats_select_features in module selection_utils:\n",
      "\n",
      "stats_select_features(relevance_table)\n",
      "    Using a table with the statistical significance of each feature,\n",
      "    returns only low-correlated relevant features.\n",
      "\n",
      "    It is assumed that the correlated attributes are calls of the same\n",
      "    function with different parameters. Therefore, all the features are\n",
      "    factorized by the values of the function arguments, and from each class\n",
      "    the representative with the lowest ``p_value`` is selected. Because the\n",
      "    table is sorted by ``p_value``, factorization is easy to implement\n",
      "    through a set.\n",
      "\n",
      "    :param relevance_table: pd.DataFrame: a table with the calculated features and their statistical significance\n",
      "    :return: List[str]: a list of names of relevant low-correlated features from ``relevance_table``.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.stats_select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45df4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# stats selected features\n",
    "stats_start_time = time.time()\n",
    "relevance_table = selection_utils.get_stats(train_list, n_jobs=n_jobs)\n",
    "stats_selected_features = selection_utils.stats_select_features(relevance_table)\n",
    "stats_train = [df[stats_selected_features + ['target']][:-test_size] for df in train_list]\n",
    "stats_test = [df[stats_selected_features + ['target']][-test_size:] for df in train_list]\n",
    "stats_models = selection_utils.get_fitted_models(stats_train, n_jobs)\n",
    "stats_rmse = get_rmse(stats_models, stats_test)\n",
    "stats_time = time.time() - stats_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116031f",
   "metadata": {},
   "source": [
    "The last step in dimensionality reduction will be `feature_importance` counting and subsequent selection by these indicators. We will use five basic 5 importances from ``XGBRegressor`` ``('gain', 'weight', 'cover', 'total_gain', 'total_cover')``, and we will take the 6th as the ``shap`` importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6edcb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_importance in module selection_utils:\n",
      "\n",
      "get_importance(models, train_list, mode='all')\n",
      "    Using the built-in feature importance estimation methods within ``XGBRegressor``\n",
      "    and the shap algorithm, it calculates the importance of the features on all\n",
      "    training data, normalizes and averages them.\n",
      "\n",
      "    :param models: List[xgboost.sklearn.XGBRegressor]: the list of trained models\n",
      "    :param train_list: List[pd.DataFrame]: the list of training data\n",
      "    :param mode: str:  importance calculating mode\n",
      "    :return: Dict[str, float]: dictionary, its keys are the features from the training data,\n",
      "     and the values are the calculated importance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.get_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dda5e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function importance_select_features in module selection_utils:\n",
      "\n",
      "importance_select_features(importance_dict, portion=0.8)\n",
      "    According to the values of the importance of the attributes selects\n",
      "    the best of them, which contain the ``portion`` % of the importance\n",
      "    of all the features.\n",
      "\n",
      "    :param importance_dict: Dict[str, float]: a dictionary with the importance of each feature\n",
      "    :param portion: float: portion of the importance of all the features to be ensured\n",
      "    :return: List[Tuple[str, float]]: a minimum number of features, the overall importance of which >= ``portion``\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.importance_select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31cc40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# importance selected features (we can use stats models to calculate importances)\n",
    "imps_start_time = time.time()\n",
    "importances = selection_utils.get_importance(stats_models, stats_train, mode='all') \n",
    "imps_selected_features = [el[0] for el in selection_utils.importance_select_features(importances, 0.8)]\n",
    "imps_train = [df[imps_selected_features + ['target']][:-test_size] for df in train_list]\n",
    "imps_test = [df[imps_selected_features + ['target']][-test_size:] for df in train_list]\n",
    "imps_models = selection_utils.get_fitted_models(imps_train, n_jobs)\n",
    "imps_rmse = get_rmse(imps_models, imps_test)\n",
    "imps_time = time.time() - imps_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a7fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rmse\n",
    "\n",
    "# average of all models\n",
    "avg_def_rmse = np.hstack(def_rmse).mean(axis=1)\n",
    "avg_stats_rmse = np.hstack(stats_rmse).mean(axis=1)\n",
    "avg_imps_rmse = np.hstack(imps_rmse).mean(axis=1)\n",
    "\n",
    "\n",
    "# average of all prediction lengths\n",
    "global_def_rmse = avg_def_rmse.mean()\n",
    "global_stats_rmse = avg_stats_rmse.mean()\n",
    "global_imps_rmse = avg_imps_rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e5017dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>RMSE mean</th>\n",
       "      <th>RMSE l=1</th>\n",
       "      <th>RMSE l=2</th>\n",
       "      <th>RMSE l=3</th>\n",
       "      <th>RMSE l=4</th>\n",
       "      <th>RMSE l=5</th>\n",
       "      <th>RMSE l=6</th>\n",
       "      <th>RMSE l=7</th>\n",
       "      <th>RMSE l=8</th>\n",
       "      <th>RMSE l=9</th>\n",
       "      <th>RMSE l=10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all features</th>\n",
       "      <td>10.2</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats selection</th>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats+imp selection</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0182</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.0220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time (s)  RMSE mean  RMSE l=1  RMSE l=2  RMSE l=3  \\\n",
       "all features             10.2     0.0183    0.0145    0.0156    0.0169   \n",
       "stats selection           1.6     0.0186    0.0157    0.0154    0.0176   \n",
       "stats+imp selection       1.1     0.0201    0.0169    0.0182    0.0196   \n",
       "\n",
       "                     RMSE l=4  RMSE l=5  RMSE l=6  RMSE l=7  RMSE l=8  \\\n",
       "all features           0.0169    0.0175    0.0185    0.0200    0.0197   \n",
       "stats selection        0.0195    0.0195    0.0195    0.0194    0.0194   \n",
       "stats+imp selection    0.0200    0.0207    0.0208    0.0206    0.0205   \n",
       "\n",
       "                     RMSE l=9  RMSE l=10  \n",
       "all features           0.0215     0.0219  \n",
       "stats selection        0.0201     0.0205  \n",
       "stats+imp selection    0.0214     0.0220  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize\n",
    "rmses = [f'RMSE l={i}' for i in range(1,11)]\n",
    "results = pd.DataFrame(columns = ['Time (s)', 'RMSE mean'] + rmses)\n",
    "\n",
    "num_models = 24\n",
    "\n",
    "results.loc['all features'] = [def_time/num_models]+[global_def_rmse]+list(avg_def_rmse)\n",
    "results.loc['stats selection'] = [stats_time/num_models]+[global_stats_rmse]+list(avg_stats_rmse)\n",
    "results.loc['stats+imp selection'] = [imps_time/num_models]+[global_imps_rmse]+list(avg_imps_rmse)\n",
    "\n",
    "\n",
    "results.values[:] = results.values.round(4)\n",
    "results['Time (s)'] = results['Time (s)'].round(1)\n",
    "\n",
    "results.to_csv('../docs/tables/building_features_results.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5f58e",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "We can see that the features selected through statistical tests allow us to reduce training time by a factor of `7`, while losing ~0% in accuracy over the next `300ms`.\n",
    "\n",
    "With feature importance we reduce the learning time of the model by a factor of `10` losing in accuracy 10%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481b733",
   "metadata": {},
   "source": [
    "Now we know which statistical features from `tsfresh` were the most useful for predicting the target. This will help us to make great optimization in the future - when we take into account information about trades in other currencies, we will only calculate **these features**, thus saving a huge amount of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6470259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfresh\n",
    "tsfresh_features = [f for f in imps_selected_features if 'price_mean__' in f]\n",
    "fc_parameters = tsfresh.feature_extraction.settings.from_columns(tsfresh_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f605eab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price_mean': {'mean_second_derivative_central': None,\n",
       "  'augmented_dickey_fuller': [{'attr': 'pvalue', 'autolag': 'AIC'}],\n",
       "  'partial_autocorrelation': [{'lag': 3}],\n",
       "  'fft_coefficient': [{'attr': 'imag', 'coeff': 70}],\n",
       "  'cwt_coefficients': [{'coeff': 0, 'w': 2, 'widths': (2, 5, 10, 20)}],\n",
       "  'ar_coefficient': [{'coeff': 8, 'k': 10}],\n",
       "  'skewness': None,\n",
       "  'binned_entropy': [{'max_bins': 10}],\n",
       "  'spkt_welch_density': [{'coeff': 2}],\n",
       "  'friedrich_coefficients': [{'coeff': 3, 'm': 3, 'r': 30}],\n",
       "  'autocorrelation': [{'lag': 3}],\n",
       "  'permutation_entropy': [{'dimension': 3, 'tau': 1}],\n",
       "  'mean_abs_change': None,\n",
       "  'fft_aggregated': [{'aggtype': 'variance'}],\n",
       "  'max_langevin_fixed_point': [{'m': 3, 'r': 30}],\n",
       "  'kurtosis': None,\n",
       "  'energy_ratio_by_chunks': [{'num_segments': 10, 'segment_focus': 0}],\n",
       "  'change_quantiles': [{'f_agg': 'mean', 'isabs': True, 'qh': 0.8, 'ql': 0.0},\n",
       "   {'f_agg': 'mean', 'isabs': False, 'qh': 0.2, 'ql': 0.0}],\n",
       "  'mean_change': None,\n",
       "  'time_reversal_asymmetry_statistic': [{'lag': 3}],\n",
       "  'linear_trend': [{'attr': 'slope'}],\n",
       "  'agg_autocorrelation': [{'f_agg': 'mean', 'maxlag': 40}],\n",
       "  'cid_ce': [{'normalize': True}, {'normalize': False}],\n",
       "  'agg_linear_trend': [{'attr': 'slope', 'chunk_len': 50, 'f_agg': 'max'}],\n",
       "  'root_mean_square': None,\n",
       "  'variation_coefficient': None,\n",
       "  'count_above_mean': None,\n",
       "  'longest_strike_below_mean': None,\n",
       "  'longest_strike_above_mean': None,\n",
       "  'last_location_of_maximum': None}}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d277942e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(fc_parameters['price_mean'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418e6a2",
   "metadata": {},
   "source": [
    "In the final `features_selection` notebook we will use information about prices of other currencies. Information about which features are the most useful will help us significantly reduce training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
