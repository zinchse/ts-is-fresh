{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "951b03f9",
   "metadata": {},
   "source": [
    "In the previous `preprocessing_notebook`, we created **separate** tables for each of the currencies. We then **quantized** the time series using aggregations over a `300ms` time window. This **decreased** the size of the data, **unified** the data format for all currencies, and **reduced** the problem to predicting the average price in the next `300ms` window.\n",
    "\n",
    "In this `building_features_notebook` we will solve the problem of **building** new useful features. To do this, we will explore popular approaches to feature creation, and, given the constraints of our task, build the best approach. **First**, we will understand which features will be useful from the **target** time series. In the **next** `selecting_features_notebook` we will use information on **other currencies** and, finally, **select** the most useful features!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e36ce44",
   "metadata": {},
   "source": [
    "# Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467efd1d",
   "metadata": {},
   "source": [
    "I know 4 basic ways to solve the problem:\n",
    "\n",
    "**A) smart feature engineering**: using domain knowledge, important features are created by hand, over which a simple (e.g., linear) model is then built \n",
    "> easy to further train on-line \\\n",
    "> it's interpretable\\\n",
    "> very fast model inference\\\n",
    "> domain knowledge is needed\n",
    "\n",
    "**B) semi-automatic feature engineering**: using some heuristics, different kinds of statistics (medians, quantiles, etc.) are computed, over which then treebased models are built\n",
    "\n",
    "> less demanding of domain knowledge (because of the use of a more complex model, we can afford to build less expressive features)\\\n",
    "> high expressive power\\\n",
    "> fast model inference\\\n",
    "> cannot be quickly retrained on-line\\ \n",
    "> it's uninterpretable\n",
    "\n",
    "**C) statistical autoregressive approach**: models like Arima, Prophet, etc.\n",
    "\n",
    "> fast model inference\\\n",
    "> correct selection of hyperparameters is necessary to build a good model\n",
    "\n",
    "**D) RNN-like approaches:** recurrent neural networks like LSTM and others\n",
    "\n",
    "> very heavy models (in terms of training and inference)\\\n",
    "> can show very good results\n",
    "\n",
    "Due to my limited knowledge of the cryptocurrency market, I am removing the **A)** option. Since we have a lot of data, it will be quite hard to train high quality statistical models (to enumerate hyperparameters). Because of this approach **C)** is also rejected. Our goal is to predict `300ms` ahead, because of the fact that in approach **D)** this is comparable to inference models, it is also removed.\n",
    "\n",
    "This leaves approach **B)**, in which we need to automatically construct good features. Moreover, because of the limitation on inference and the lack of on-line retraining, our model must work fast enough (there must not be very many features), and also have a prediction horizon comparable to the learning time of the new model *(we must have a good model at every moment, if the model is built longer than its predictions become obsolete, we will not be able to trade).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9887a14",
   "metadata": {},
   "source": [
    "# Solution description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3e9185",
   "metadata": {},
   "source": [
    "**Target:** predict the price of the target currency for the next `300ms`\n",
    "\n",
    "**Input:** transaction history for a set of currencies (including the target one)\n",
    "\n",
    "**Limitations:** fast inference and model learning; small number of features\n",
    "\n",
    "First, we will generate a huge number of statistical features (based on the target table). Then we will use statistical hypotheses to test whether they are important in the forecasting. We discard all unimportant features, and thus obtain `stats_selected_features`. This part will be solved using methods from `tsfresh` library and *block cross validation* of time series. I use block cross-validation to test the importance of features in different time conditions within the same day. You can read more about what block cross validation is in the **DOCS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261c3954",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import extraction_utils\n",
    "import preprocessing_utils\n",
    "from tsfresh.feature_extraction import EfficientFCParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f142d",
   "metadata": {},
   "source": [
    "**Text from the docstring of `extraction_utils.bcv_extract_features`:**\n",
    "\n",
    "---\n",
    "\n",
    "Implement the process of block cross validation of time series with\n",
    "counting of window features within each block:\n",
    "\n",
    "- divide the entire dataframe evenly into `n_tests` blocks\n",
    "\n",
    "- inside each block calculate statistics on `n_windows` windows of size `windows_size`\n",
    "\n",
    "- as a target column make % change in `target_col` to simplify the task for the tree based models\n",
    "\n",
    "- creates lag features with numbers from `lags`\n",
    "\n",
    "\n",
    "Depending on the `mode`, window feature evaluations are satisfactorily\n",
    "either using the `tsfresh` methods (parallel mode) or using the\n",
    "`window_featurizing` method (default mode). The difference is that the first method can be parallelized, but in\n",
    "the process of execution, the size of memory can increase many times over\n",
    "(due to `roll_time_series` function). In the second method, the amount of\n",
    "memory does not grow, but it will not work to parallelize the process.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7872b84",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "names = np.array([\n",
    "    '1000LUNC_USDT_PERP', '1000SHIB_USDT_PERP', '1000XEC_USDT_PERP',\n",
    "    '1INCH_USDT_PERP', 'AAVE_USDT_PERP', 'ADA_BUSD_PERP', 'ADA_USDT_PERP',\n",
    "    'ALGO_USDT_PERP', 'ALICE_USDT_PERP', 'ALPHA_USDT_PERP', 'ANC_BUSD_PERP',\n",
    "    'ANKR_USDT_PERP', 'ANT_USDT_PERP', 'APE_BUSD_PERP', 'APE_USDT_PERP',\n",
    "    'API3_USDT_PERP', 'APT_USDT_PERP', 'ARPA_USDT_PERP', 'AR_USDT_PERP',\n",
    "    'ATA_USDT_PERP', 'ATOM_USDT_PERP', 'AUDIO_USDT_PERP', 'AVAX_BUSD_PERP',\n",
    "    'AVAX_USDT_PERP', 'AXS_USDT_PERP', 'BAKE_USDT_PERP', 'BAL_USDT_PERP',\n",
    "    'BAND_USDT_PERP', 'BAT_USDT_PERP', 'BCH_USDT_PERP', 'BEL_USDT_PERP',\n",
    "    'BLZ_USDT_PERP', 'BNB_BUSD_PERP', 'BNB_USDT_PERP', 'BNX_USDT_PERP',\n",
    "    'BTCDOM_USDT_PERP', 'BTC_BUSD_PERP', 'BTC_USDT_CQ', 'BTC_USDT_PERP',\n",
    "    'C98_USDT_PERP', 'CELO_USDT_PERP', 'CELR_USDT_PERP', 'CHR_USDT_PERP',\n",
    "    'CHZ_USDT_PERP', 'COMP_USDT_PERP', 'COTI_USDT_PERP', 'CRV_USDT_PERP',\n",
    "    'CTK_USDT_PERP', 'CTSI_USDT_PERP', 'CVC_USDT_PERP', 'CVX_USDT_PERP',\n",
    "    'DAR_USDT_PERP', 'DASH_USDT_PERP', 'DEFI_USDT_PERP', 'DENT_USDT_PERP',\n",
    "    'DGB_USDT_PERP', 'DODO_BUSD_PERP', 'DOGE_BUSD_PERP', 'DOGE_USDT_PERP',\n",
    "    'DOT_BUSD_PERP', 'DOT_USDT_PERP', 'DUSK_USDT_PERP', 'DYDX_USDT_PERP',\n",
    "    'EGLD_USDT_PERP', 'ENJ_USDT_PERP', 'ENS_USDT_PERP', 'EOS_USDT_PERP',\n",
    "    'ETC_BUSD_PERP', 'ETC_USDT_PERP', 'ETH_BUSD_PERP', 'ETH_USDT_CQ',\n",
    "    'ETH_USDT_PERP', 'FIL_BUSD_PERP', 'FIL_USDT_PERP', 'FLM_USDT_PERP',\n",
    "    'FLOW_USDT_PERP', 'FOOTBALL_USDT_PERP', 'FTM_BUSD_PERP', 'FTM_USDT_PERP',\n",
    "    'GALA_BUSD_PERP', 'GALA_USDT_PERP', 'GAL_BUSD_PERP', 'GAL_USDT_PERP',\n",
    "    'GMT_BUSD_PERP', 'GMT_USDT_PERP', 'GRT_USDT_PERP', 'GTC_USDT_PERP',\n",
    "    'HBAR_USDT_PERP', 'HNT_USDT_PERP', 'HOT_USDT_PERP', 'ICP_BUSD_PERP',\n",
    "    'ICP_USDT_PERP', 'ICX_USDT_PERP', 'IMX_USDT_PERP', 'INJ_USDT_PERP',\n",
    "    'IOST_USDT_PERP', 'IOTA_USDT_PERP', 'IOTX_USDT_PERP', 'JASMY_USDT_PERP',\n",
    "    'KAVA_USDT_PERP', 'KLAY_USDT_PERP', 'KNC_USDT_PERP', 'KSM_USDT_PERP',\n",
    "    'LDO_USDT_PERP', 'LINA_USDT_PERP', 'LINK_BUSD_PERP', 'LINK_USDT_PERP',\n",
    "    'LIT_USDT_PERP', 'LPT_USDT_PERP', 'LRC_USDT_PERP', 'LTC_USDT_PERP',\n",
    "    'LUNA2_USDT_PERP', 'MANA_USDT_PERP', 'MASK_USDT_PERP', 'MATIC_BUSD_PERP',\n",
    "    'MATIC_USDT_PERP', 'MKR_USDT_PERP', 'MTL_USDT_PERP', 'NEAR_BUSD_PERP',\n",
    "    'NEAR_USDT_PERP', 'NEO_USDT_PERP', 'NKN_USDT_PERP', 'OCEAN_USDT_PERP',\n",
    "    'OGN_USDT_PERP', 'OMG_USDT_PERP', 'ONE_USDT_PERP', 'ONT_USDT_PERP',\n",
    "    'OP_USDT_PERP', 'PEOPLE_USDT_PERP', 'QNT_USDT_PERP', 'QTUM_USDT_PERP',\n",
    "    'RAY_USDT_PERP', 'REEF_USDT_PERP', 'REN_USDT_PERP', 'RLC_USDT_PERP',\n",
    "    'ROSE_USDT_PERP', 'RSR_USDT_PERP', 'RUNE_USDT_PERP', 'RVN_USDT_PERP',\n",
    "    'SAND_USDT_PERP', 'SFP_USDT_PERP', 'SKL_USDT_PERP', 'SNX_USDT_PERP',\n",
    "    'SOL_BUSD_PERP', 'SOL_USDT_PERP', 'SPELL_USDT_PERP', 'SRM_USDT_PERP',\n",
    "    'STG_USDT_PERP', 'STMX_USDT_PERP', 'STORJ_USDT_PERP', 'SUSHI_USDT_PERP',\n",
    "    'SXP_USDT_PERP', 'THETA_USDT_PERP', 'TOMO_USDT_PERP', 'TRB_USDT_PERP',\n",
    "    'TRX_BUSD_PERP', 'TRX_USDT_PERP', 'UNFI_USDT_PERP', 'UNI_BUSD_PERP',\n",
    "    'UNI_USDT_PERP', 'VET_USDT_PERP', 'WAVES_BUSD_PERP', 'WAVES_USDT_PERP',\n",
    "    'WOO_USDT_PERP', 'XEM_USDT_PERP', 'XLM_USDT_PERP', 'XMR_USDT_PERP',\n",
    "    'XRP_BUSD_PERP', 'XRP_USDT_PERP', 'XTZ_USDT_PERP', 'YFI_USDT_PERP',\n",
    "    'ZEC_USDT_PERP', 'ZEN_USDT_PERP', 'ZIL_USDT_PERP', 'ZRX_USDT_PERP'\n",
    "],\n",
    "                 dtype=object)\n",
    "TARGET_NAME = 'CHZ_USDT_PERP_MIDPRICE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35de4dae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "quantized_df_dict = preprocessing_utils.load_tables(names=list(names)+[TARGET_NAME],\n",
    "                                              path_from='../data/quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "541cd32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_table = quantized_df_dict[TARGET_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084fda83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>event_time</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-11-15 00:00:00.000</th>\n",
       "      <td>0.198392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15 00:00:00.300</th>\n",
       "      <td>0.198341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-11-15 00:00:00.600</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         price_mean\n",
       "event_time                         \n",
       "2022-11-15 00:00:00.000    0.198392\n",
       "2022-11-15 00:00:00.300    0.198341\n",
       "2022-11-15 00:00:00.600         NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_table.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fca4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adce32c",
   "metadata": {},
   "source": [
    "# Block cross validation and feature building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b2ec6e",
   "metadata": {},
   "source": [
    "Let's do a block cross-validation. The number of blocks will be 24 (approximately one block for each hour). For each block we will take 1800 (this is approximately the last 9 minutes) windows with a width of a minute.\n",
    "\n",
    "Since we have not yet reduced the dimensionality of the features to be counted (`tsfresh` generates about 800), the next step takes quite a long time. \\\n",
    "*You can go get a coffee :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d321080",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_list = extraction_utils.bcv_extract_features(\n",
    "    df=target_table,\n",
    "    n_blocks=24, \n",
    "    target_col='price_mean',\n",
    "    n_jobs=8,\n",
    "    n_windows=1800,\n",
    "    window_size=200,\n",
    "    lags=list(range(1,11)),\n",
    "    mode='parallel',\n",
    "    fc_parameters=EfficientFCParameters()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c653bece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 793)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468cf1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 10\n",
    "n_jobs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c1fa847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selection_utils\n",
    "from tsfresh.feature_selection.relevance import calculate_relevance_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a507c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_fitted_models in module selection_utils:\n",
      "\n",
      "get_fitted_models(train_list: List[pandas.core.frame.DataFrame], n_jobs=8) -> List[xgboost.sklearn.XGBRegressor]\n",
      "    Returns the trained model for each `train_list` dataframe.\n",
      "    \n",
      "    :param train_list: training data list\n",
      "    :param n_jobs: number of cores for parallel learning\n",
      "    :return: list fitted `XGBRegressor` models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.get_fitted_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baffcf0",
   "metadata": {},
   "source": [
    "Note that the time interval of our predictions is `300ms`. At the same time we are not able to train our model on-line. This leads us to think that the model we built will be used in practice for **more time**. So we need to estimate the quality of its predictions not only for the next `300ms`, but for several steps ahead. \n",
    "\n",
    "For simplicity, we will take 10 intervals of `300ms`, that is, our model will predict the prices behavior for the next 3 seconds. During this time we are already able to train a new model on a part of the received data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0184d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import xgboost\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "899cdd75",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_rmse(\n",
    "        models: List[xgboost.sklearn.XGBRegressor],\n",
    "        test_list: List[pd.DataFrame]\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Predictions are made for each model in the `models` and the corresponding\n",
    "    training dataset. Then rmse are calculated for all lengths of the\n",
    "    prediction horizon from 1 to `len(test_list)`. That is, we look at the\n",
    "    rmse of the predictions for the next `300ms`, the next `600ms`, and so on.  \n",
    "    \n",
    "    :param models: list of trained XGBRegressors\n",
    "    :param test_list: test data list\n",
    "    :return: list containing the list with rmse for all prediction horizons\n",
    "    \"\"\"\n",
    "    assert len(models) == len(test_list), f'len(models)={len(models)}!={len(test_list)}=len(test_list)!'\n",
    "\n",
    "    length = test_list[0].shape[0]\n",
    "\n",
    "    losses = []\n",
    "    for i, model in enumerate(models):\n",
    "        losses.append([])\n",
    "        x, y = test_list[i].drop(['target'], axis=1), test_list[i].loc[:, 'target']\n",
    "        mse = .0\n",
    "        for l in range(length):\n",
    "            y_hat = model.predict(x.iloc[l:l + 1])\n",
    "            mse += (y_hat - y.iloc[l]) ** 2\n",
    "            losses[-1].append(np.sqrt(mse / (l + 1)))\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9ba16",
   "metadata": {},
   "source": [
    "# Size reduction\n",
    "\n",
    "### using some techniques, we will select the most important features among all the generated `tsfresh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88db0a8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# default features from tsfresh\n",
    "def_start_time = time.time()\n",
    "def_train = [df[:-test_size] for df in train_list]\n",
    "def_test = [df[-test_size:] for df in train_list]\n",
    "def_models = selection_utils.get_fitted_models(def_train, n_jobs)\n",
    "def_rmse = get_rmse(def_models, def_test)\n",
    "def_time = time.time() - def_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575f162",
   "metadata": {},
   "source": [
    "Now we will try to reduce the dimensionality of the feature space by calculating the statistical significance of each of them. After that we will select the uncorrelated features with the highest `p_value`. This will help us reduce the dimensionality of the space even more, and will also help us use the `feature_importance` technique in the future. See **DOCS** for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16d8e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_stats in module selection_utils:\n",
      "\n",
      "get_stats(blocks: List[pandas.core.frame.DataFrame], n_jobs: int = 1) -> pandas.core.frame.DataFrame\n",
      "    Using statistical criteria, calculates the significance of the features\n",
      "     for each block in the list. Then the obtained `p_value`s are averaged.\n",
      "    \n",
      "    :param blocks: list of datas with `target` column and the same scheme\n",
      "    :param n_jobs: the number of cores that can be used in the calculation of stat values\n",
      "    :return: df with calculated p_value for each of the attributes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.get_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2883992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stats_select_features in module selection_utils:\n",
      "\n",
      "stats_select_features(relevance_table: pandas.core.frame.DataFrame) -> List[ForwardRef('str')]\n",
      "    Using a table with the statistical significance of each feature,\n",
      "    returns only low-correlated relevant features.\n",
      "    \n",
      "    It is assumed that the correlated attributes are calls of the same\n",
      "    function with different parameters. Therefore, all the features are\n",
      "    factorized by the values of the function arguments, and from each class\n",
      "    the representative with the lowest `p_value` is selected. Because the\n",
      "    table is sorted by `p_value`, factorization is easy to implement\n",
      "    through a set.\n",
      "    \n",
      "    :param relevance_table: a table with the calculated features and their\n",
      "     statistical significance\n",
      "    :return: a list of names of relevant low-correlated features from\n",
      "     `relevance_table`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.stats_select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a45df4d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# stats selected features\n",
    "stats_start_time = time.time()\n",
    "relevance_table = selection_utils.get_stats(train_list, n_jobs=n_jobs)\n",
    "stats_selected_features = selection_utils.stats_select_features(relevance_table)\n",
    "stats_train = [df[stats_selected_features + ['target']][:-test_size] for df in train_list]\n",
    "stats_test = [df[stats_selected_features + ['target']][-test_size:] for df in train_list]\n",
    "stats_models = selection_utils.get_fitted_models(stats_train, n_jobs)\n",
    "stats_rmse = get_rmse(stats_models, stats_test)\n",
    "stats_time = time.time() - stats_start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116031f",
   "metadata": {},
   "source": [
    "The last step in dimensionality reduction will be `feature_importance` counting and subsequent selection by these indicators. We will use five basic 5 importances from XGBRegressor ('gain', 'weight', 'cover', 'total_gain', 'total_cover'), and we will take the 6th as the `shap` importance. See the **DOCS** for details on how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6edcb11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_importance in module selection_utils:\n",
      "\n",
      "get_importance(models: List[xgboost.sklearn.XGBRegressor], train_list: List[pandas.core.frame.DataFrame], mode: str = 'all') -> Dict[str, float]\n",
      "    Using the built-in feature importance estimation methods within `XGBRegressor`\n",
      "    and the shap algorithm, it calculates the importance of the features on all\n",
      "    training data, normalizes and averages them.\n",
      "    \n",
      "    :param models: the list of trained models\n",
      "    :param train_list: the list of training data\n",
      "    :param mode: importance calculating mode\n",
      "    :return: dictionary, its keys are the features from the training data,\n",
      "     and the values are the calculated importance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.get_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda5e77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function importance_select_features in module selection_utils:\n",
      "\n",
      "importance_select_features(importance_dict: Dict[str, float], portion: float = 0.8) -> List[Tuple[str, float]]\n",
      "    According to the values of the importance of the attributes selects\n",
      "    the best of them, which contain the `portion` % of the importance\n",
      "    of all the features.\n",
      "    \n",
      "    :param importance_dict: a dictionary with the importance of each feature\n",
      "    :param portion: portion of the importance of all the features to be ensured\n",
      "    :return: a minimum number of features, the overall importance of which >= portion\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(selection_utils.importance_select_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31cc40",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# importance selected features (we can use stats models to calculate importances)\n",
    "imps_start_time = time.time()\n",
    "importances = selection_utils.get_importance(stats_models, stats_train, mode='all') \n",
    "imps_selected_features = [el[0] for el in selection_utils.importance_select_features(importances, 0.8)]\n",
    "imps_train = [df[imps_selected_features + ['target']][:-test_size] for df in train_list]\n",
    "imps_test = [df[imps_selected_features + ['target']][-test_size:] for df in train_list]\n",
    "imps_models = selection_utils.get_fitted_models(imps_train, n_jobs)\n",
    "imps_rmse = get_rmse(imps_models, imps_test)\n",
    "imps_time = time.time() - imps_start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a7fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rmse\n",
    "\n",
    "# average of all models\n",
    "avg_def_rmse = np.hstack(def_rmse).mean(axis=1)\n",
    "avg_stats_rmse = np.hstack(stats_rmse).mean(axis=1)\n",
    "avg_imps_rmse = np.hstack(imps_rmse).mean(axis=1)\n",
    "\n",
    "\n",
    "# average of all prediction lengths\n",
    "global_def_rmse = avg_def_rmse.mean()\n",
    "global_stats_rmse = avg_stats_rmse.mean()\n",
    "global_imps_rmse = avg_imps_rmse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5017dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time (s)</th>\n",
       "      <th>RMSE mean</th>\n",
       "      <th>RMSE l=1</th>\n",
       "      <th>RMSE l=2</th>\n",
       "      <th>RMSE l=3</th>\n",
       "      <th>RMSE l=4</th>\n",
       "      <th>RMSE l=5</th>\n",
       "      <th>RMSE l=6</th>\n",
       "      <th>RMSE l=7</th>\n",
       "      <th>RMSE l=8</th>\n",
       "      <th>RMSE l=9</th>\n",
       "      <th>RMSE l=10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>all features</th>\n",
       "      <td>14.4</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0169</td>\n",
       "      <td>0.0175</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats selection</th>\n",
       "      <td>2.6</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stats+imp selection</th>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0193</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>0.0212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Time (s)  RMSE mean  RMSE l=1  RMSE l=2  RMSE l=3  \\\n",
       "all features             14.4     0.0183    0.0145    0.0156    0.0169   \n",
       "stats selection           2.6     0.0195    0.0168    0.0168    0.0186   \n",
       "stats+imp selection       2.2     0.0186    0.0158    0.0161    0.0176   \n",
       "\n",
       "                     RMSE l=4  RMSE l=5  RMSE l=6  RMSE l=7  RMSE l=8  \\\n",
       "all features           0.0169    0.0175    0.0185    0.0200    0.0197   \n",
       "stats selection        0.0204    0.0202    0.0202    0.0202    0.0202   \n",
       "stats+imp selection    0.0186    0.0188    0.0193    0.0190    0.0192   \n",
       "\n",
       "                     RMSE l=9  RMSE l=10  \n",
       "all features           0.0215     0.0219  \n",
       "stats selection        0.0208     0.0212  \n",
       "stats+imp selection    0.0204     0.0212  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize\n",
    "rmses = [f'RMSE l={i}' for i in range(1,11)]\n",
    "results = pd.DataFrame(columns = ['Time (s)', 'RMSE mean'] + rmses)\n",
    "\n",
    "num_models = 24\n",
    "\n",
    "results.loc['all features'] = [def_time/num_models]+[global_def_rmse]+list(avg_def_rmse)\n",
    "results.loc['stats selection'] = [stats_time/num_models]+[global_stats_rmse]+list(avg_stats_rmse)\n",
    "results.loc['stats+imp selection'] = [imps_time/num_models]+[global_imps_rmse]+list(avg_imps_rmse)\n",
    "\n",
    "\n",
    "results.values[:] = results.values.round(4)\n",
    "results['Time (s)'] = results['Time (s)'].round(1)\n",
    "\n",
    "results.to_csv('../docs/tables/building_features_results.csv')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5f58e",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "We can see that the features selected through statistical tests allow us to reduce training time by a factor of 6, while losing 4.4% in accuracy on average across all horizons, and 13% over the next `300ms`.\n",
    "\n",
    "With feature importance we reduce the learning time of the model by a factor of 8.3, losing in accuracy 11.3% and 17.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481b733",
   "metadata": {},
   "source": [
    "Now we know which statistical features from `tsfresh` were the most useful for predicting the target. This will help us to make great optimization in the future - when we take into account information about trades in other currencies, we will only calculate **these features**, thus saving a huge amount of resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6470259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfresh\n",
    "tsfresh_features = [f for f in imps_selected_features if 'price_mean__' in f]\n",
    "fc_parameters = tsfresh.feature_extraction.settings.from_columns(tsfresh_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f605eab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price_mean': {'mean_second_derivative_central': None,\n",
       "  'cwt_coefficients': [{'coeff': 0, 'w': 2, 'widths': (2, 5, 10, 20)}],\n",
       "  'partial_autocorrelation': [{'lag': 3}],\n",
       "  'fft_coefficient': [{'attr': 'imag', 'coeff': 70}],\n",
       "  'variation_coefficient': None,\n",
       "  'fft_aggregated': [{'aggtype': 'variance'}],\n",
       "  'ar_coefficient': [{'coeff': 8, 'k': 10}],\n",
       "  'augmented_dickey_fuller': [{'attr': 'pvalue', 'autolag': 'AIC'}],\n",
       "  'friedrich_coefficients': [{'coeff': 3, 'm': 3, 'r': 30}],\n",
       "  'spkt_welch_density': [{'coeff': 2}],\n",
       "  'binned_entropy': [{'max_bins': 10}],\n",
       "  'permutation_entropy': [{'dimension': 3, 'tau': 1}],\n",
       "  'autocorrelation': [{'lag': 3}],\n",
       "  'max_langevin_fixed_point': [{'m': 3, 'r': 30}],\n",
       "  'skewness': None,\n",
       "  'energy_ratio_by_chunks': [{'num_segments': 10, 'segment_focus': 0}],\n",
       "  'kurtosis': None,\n",
       "  'change_quantiles': [{'f_agg': 'mean', 'isabs': True, 'qh': 0.8, 'ql': 0.0},\n",
       "   {'f_agg': 'mean', 'isabs': False, 'qh': 0.2, 'ql': 0.0}],\n",
       "  'time_reversal_asymmetry_statistic': [{'lag': 3}],\n",
       "  'agg_autocorrelation': [{'f_agg': 'mean', 'maxlag': 40}],\n",
       "  'mean_change': None,\n",
       "  'absolute_sum_of_changes': None,\n",
       "  'cid_ce': [{'normalize': True}, {'normalize': False}],\n",
       "  'agg_linear_trend': [{'attr': 'slope', 'chunk_len': 50, 'f_agg': 'max'}],\n",
       "  'linear_trend': [{'attr': 'slope'}],\n",
       "  'abs_energy': None,\n",
       "  'count_below_mean': None,\n",
       "  'longest_strike_above_mean': None,\n",
       "  'longest_strike_below_mean': None,\n",
       "  'ratio_beyond_r_sigma': [{'r': 1.5}]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d277942e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(fc_parameters['price_mean'].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3418e6a2",
   "metadata": {},
   "source": [
    "In the final `selection_features_notebook` notebook we will use information about prices of other currencies. Information about which features are the most useful will help us significantly reduce training time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
